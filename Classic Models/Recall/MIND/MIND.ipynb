{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# MIND (Multi-Interest Network with Dynamic Routing)\n",
    "\n",
    "**MINDçš„åŸºæœ¬æ€æƒ³**ï¼šä½¿ç”¨**å¤šä¸ª**å‘é‡è€Œä¸æ˜¯ä¸€ä¸ªå•ä¸€å‘é‡æ¥è¡¨ç¤ºç”¨æˆ·çš„å¤šä¸ªå…´è¶£å±‚é¢ï¼ˆæ¯”å¦‚ï¼Œç”¨æˆ·å¯èƒ½æ—¢å¯¹æœè£…æ„Ÿå…´è¶£åˆå¯¹æ•°ç äº§å“æ„Ÿå…´è¶£ï¼‰ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒä¼šè‡ªåŠ¨æŠŠç”¨æˆ·è¡Œä¸ºæŒ‰ç…§å…´è¶£ç±»å‹è¿›è¡Œåˆ†ç»„ï¼Œå¹¶ä¸ºæ¯ç»„å…´è¶£ç”Ÿæˆä¸€ä¸ªä¸“é—¨çš„å…´è¶£å‘é‡ã€‚\n",
    "\n",
    "MINDçš„æ•´ä½“ç½‘ç»œæ¶æ„ç»„æˆï¼š\n",
    "\n",
    "- Embeddingå±‚\n",
    "- å¤šå…´è¶£æå–å±‚\n",
    "- Label-Awareæ³¨æ„åŠ›å±‚\n",
    "\n",
    "## 1. é—®é¢˜æè¿°\n",
    "\n",
    "è€ƒè™‘ç”¨æˆ·é›†åˆ $\\mathcal{U}$ å’Œ ç‰©å“é›†åˆ $\\mathcal{I}$ï¼Œåœ¨å¬å›é˜¶æ®µï¼Œéœ€è¦ä¸ºæ¯ä¸ªç”¨æˆ· $u\\in \\mathcal{U}$ ä»ä¸Šäº¿ç‰©å“ä¸­å¬å›çº¦ä¸Šåƒä¸ªç‰©å“ã€‚æ¯ä¸ªè¾“å…¥å¯è¡¨ç¤ºä¸ºä¸€ä¸ªä¸‰å…ƒç»„çš„å½¢å¼ï¼š$(\\mathcal{I_u}, \\mathcal{P_u}, \\mathcal{F_i})$ï¼Œå…¶ä¸­ $\\mathcal{I_u}$ (user behavior) è¡¨ç¤ºç”¨æˆ· $u$ äº¤äº’è¿‡çš„ç‰©å“é›†åˆï¼Œ$\\mathcal{P_u}$ (user profile) æ˜¯ç”¨æˆ·è‡ªèº«çš„ç‰¹å¾ï¼ˆå¦‚æ€§åˆ«å’Œå¹´é¾„ï¼‰ï¼Œ$\\mathcal{F_i}$ (label item) æ˜¯ç›®æ ‡ç‰©å“è‡ªèº«çš„ç‰¹å¾ï¼ˆå¦‚ç‰©å“IDå’Œç±»åˆ«IDï¼‰ã€‚\n",
    "\n",
    "*æ³¨æ„ï¼šä¸‹é¢çš„è¿™æ®µæè¿°æ˜¯ä»**æ¨ç†é˜¶æ®µ**çš„è§’åº¦æ¥é™ˆè¿°çš„ã€‚*\n",
    "\n",
    "MINDçš„æ ¸å¿ƒä»»åŠ¡æ˜¯å­¦ä¹ ä¸€ä¸ªä»åŸå§‹ç‰¹å¾åˆ°ç”¨æˆ·è¡¨å¾çš„æ˜ å°„å‡½æ•°ï¼Œå¦‚ä¸‹\n",
    "$$\n",
    "V_u = f_{user}(\\mathcal{I_u}, \\mathcal{P_u})\n",
    "$$\n",
    "\n",
    "è¿™é‡Œçš„ $V_u = \\left( \\vec{\\mathbf{v}}_u^1, \\dots, \\vec{\\mathbf{v}}_u^K \\right) \\in \\mathbb{R}^{d \\times K}$ ç”¨Kä¸ª $d$ ç»´å‘é‡è€Œä¸æ˜¯ä¸€ä¸ªå‘é‡æ¥å¯¹ç”¨æˆ·è¿›è¡Œè¡¨ç¤ºï¼ˆ$K=1$ æ—¶å°±ç±»ä¼¼äºYouTube DNNï¼‰ã€‚\n",
    "\n",
    "ç›®æ ‡ç‰©å“ $i$ åˆ™é€šè¿‡å¦ä¸€ä¸ªå‡½æ•°è¿›è¡Œæ˜ å°„ï¼Œè¡¨ç¤ºä¸º $\\vec{\\mathbf{e}}_i=f_{item}(\\mathcal{F}_i)$ï¼Œæ˜¯ä¸€ä¸ª $d$ ç»´å‘é‡ã€‚\n",
    "\n",
    "åœ¨é€šè¿‡ä¸Šè¿°æ–¹å¼åˆ†åˆ«è·å–åˆ°è¡¨ç¤ºç”¨æˆ· $u$ çš„å‘é‡ç»„å’Œè¡¨ç¤ºç‰©å“ $i$ çš„å‘é‡åï¼Œæ ¹æ®å¦‚ä¸‹æ–¹å¼å¯¹ç”¨æˆ· $u$ å’Œç‰©å“ $i$ çš„â€œè¿æ¥â€è¿›è¡Œæ‰“åˆ†ï¼Œä¸ºç”¨æˆ· $u$ å¬å›åˆ†æ•°æœ€é«˜çš„ $N$ï¼ˆ$N$ çš„å–å€¼æ˜¯é¢„å…ˆè®¾ç½®å¥½çš„ï¼‰ä¸ªå€™é€‰ç‰©å“ï¼š\n",
    "\n",
    "$$\n",
    "f_{score}(V_u, \\vec{\\mathbf{e}}_i) = \\max_{1 \\le k \\le K} \\vec{\\mathbf{e}}_i^\\mathrm{T} \\vec{\\mathbf{v}}_u^k\n",
    "$$\n",
    "\n",
    "æ³¨æ„è¿™é‡Œæœ‰ $\\max$ æ“ä½œï¼Œå› ä¸ºåº”è¯¥ç”¨ä¸ç‰©å“ $i$ æœ€ç›¸ä¼¼çš„å…´è¶£ $k$ å¯¹åº”çš„å‘é‡æ¥è®¡ç®—ç›¸ä¼¼æ€§ï¼Œæ¯”å¦‚ä¸ºç”¨æˆ· $u$ è®¡ç®—å¯¹æ‰‹æœºçš„å¬å›åˆ†æ•°æ—¶ï¼Œåº”è¯¥ç”¨æ•°ç äº§å“å¯¹åº”çš„å…´è¶£å‘é‡ï¼Œè€Œä¸æ˜¯æœè£…å¯¹åº”çš„å…´è¶£å‘é‡ï¼Œè¿™æ ·çš„è®¡ç®—ç»“æœæ‰æ˜¯æœ‰æ„ä¹‰çš„ã€‚\n",
    "\n",
    "## 2. ç½‘ç»œç»“æ„\n",
    "\n",
    "![image-20260111104858987](../../../assets/image-20260111104858987.png)\n",
    "\n",
    "### Embedding & Pooling Layer\n",
    "\n",
    "$\\mathcal{I_u}, \\mathcal{P_u}, \\mathcal{F_i}$ä¸­çš„ç‰¹å¾é€šå¸¸éƒ½åŒ…å«é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯IDç‰¹å¾ï¼‰ï¼Œå› æ­¤ä½¿ç”¨å¸¸è§çš„embeddingæŠ€æœ¯å°†è¿™äº›ç‰¹å¾åµŒå…¥åˆ°ä½ç»´çš„ç¨ å¯†å‘é‡ä¸­ï¼Œä»¥å‡å°‘å‚æ•°æ•°ç›®ï¼Œå…·ä½“å¤„ç†æ–¹å¼å¦‚ä¸‹ï¼š\n",
    "\n",
    "- å¯¹ $\\mathcal{P_u}$: ç”¨æˆ·çš„IDåŠå…¶å®ƒç”¨æˆ·ç‰¹å¾ï¼ˆå¦‚å¹´é¾„ã€æ€§åˆ«ç­‰ï¼‰ç»è¿‡embeddingåå†è¿›è¡Œ**æ‹¼æ¥** (concatenation)ï¼Œå¾—åˆ°ç”¨æˆ·å‘é‡ $\\vec{\\mathbf{p}}_u$ã€‚\n",
    "- å¯¹ $\\mathcal{F_i}$: ç‰©å“çš„IDåŠå…¶å®ƒç‰©å“ç‰¹å¾ï¼ˆå¦‚å“ç‰Œã€åº—é“ºç­‰ï¼‰ç»è¿‡embeddingåå†è¿›è¡Œ**å¹³å‡æ± åŒ–** (average pooling)ï¼Œå¾—åˆ°ç‰©å“å‘é‡ $\\vec{\\mathbf{e}}_i$ã€‚\n",
    "- å¯¹ $\\mathcal{I_u}$: ä¼šæ”¶é›†é›†åˆä¸­ç‰©å“å¯¹åº”çš„ç‰©å“å‘é‡ï¼Œå¾—åˆ°$\\mathrm{E}_u = \\{ \\vec{\\mathbf{e}}_j, j \\in \\mathcal{I}_u \\}$ã€‚\n",
    "\n",
    "### Multi-Interest Extractor Layer\n",
    "\n",
    "è¿™é‡Œçš„è®¾è®¡å€Ÿé‰´äº†èƒ¶å›Šç½‘ç»œ ([Sabour *et al.*, 2017](https://arxiv.org/pdf/1710.09829 \"\")) çš„åŠ¨æ€è·¯ç”±æœºåˆ¶ã€‚\n",
    "\n",
    "> #### åŠ¨æ€è·¯ç”±æœºåˆ¶\n",
    ">\n",
    "> åœ¨èƒ¶å›Šç½‘ç»œä¸­ï¼ŒåŠ¨æ€è·¯ç”±æœºåˆ¶çš„ç›®æ ‡æ˜¯**åœ¨ç»™å®šlow-levelçš„èƒ¶å›Šåï¼Œè®¡ç®—å‡ºhigh-levelçš„èƒ¶å›Š**ï¼ˆæ‰€è°“çš„èƒ¶å›Šå°±æ˜¯å‘é‡ï¼Œä¸åŒäºä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œä¸­æ¯ä¸ªç¥ç»å…ƒå¯¹åº”ä¸€ä¸ªæ ‡é‡ï¼Œèƒ¶å›Šç½‘ç»œä¸­æ¯ä¸ªèƒ¶å›Šå¯¹åº”ä¸€ä¸ªå‘é‡ï¼‰ã€‚\n",
    ">\n",
    "> å…·ä½“æ¥è¯´ï¼Œä¸‹å±‚æœ‰ $m$ ä¸ªèƒ¶å›Šï¼Œæ¯ä¸ªèƒ¶å›Šæ˜¯ä¸€ä¸ª $N_l$ ç»´åº¦çš„å‘é‡ï¼Œä¸Šå±‚æœ‰ $n$ ä¸ªèƒ¶å›Šï¼Œæ¯ä¸ªèƒ¶å›Šæ˜¯ä¸€ä¸ª $N_h$ ç»´åº¦çš„å‘é‡ï¼Œä¸‹å±‚ç¬¬ $i$ ä¸ªèƒ¶å›Š $\\vec{\\mathbf{c}}_i^l$ ä¸ä¸Šå±‚ç¬¬ $j$ ä¸ªèƒ¶å›Š $\\vec{\\mathbf{c}}_j^h$ ä¹‹é—´çš„åŠ¨æ€è·¯ç”±logitå°±æ˜¯\n",
    "> $$\n",
    "> b_{ij}=(\\vec{\\mathbf{c}}_j^h)^T S_{ij} \\vec{\\mathbf{c}}_i^l\n",
    "> $$\n",
    "> å…¶ä¸­ $S_{ij}$ æ˜¯è¦å­¦ä¹ çš„æƒé‡çŸ©é˜µï¼Œé¦–æ¬¡è®¡ç®—æ—¶æ²¡æœ‰ $\\vec{\\mathbf{c}}_j^h$ï¼Œå¯ä»¥ç›´æ¥å°† $b_{ij}$ åˆå§‹åŒ–ä¸º0ã€‚\n",
    ">\n",
    "> æ¯ä¸ªä¸Šå±‚çš„èƒ¶å›Š $\\vec{\\mathbf{c}}_j^h,j=1,2,...,n$ éƒ½ä¼šå—åˆ°ä¸‹å±‚æ‰€æœ‰èƒ¶å›Šçš„å½±å“ï¼Œå‰é¢å¾—åˆ°çš„logitå°±å†³å®šäº†ä¸‹å±‚ç¬¬ $i$ ä¸ªèƒ¶å›Šä¸ä¸Šå±‚ç¬¬ $j$ ä¸ªèƒ¶å›Šä¹‹é—´çš„äº¤äº’å¼ºåº¦ï¼š\n",
    "> $$\n",
    "> w_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k=1}^{m} \\exp(b_{ik})}\n",
    "> $$\n",
    "> ï¼ˆğŸ˜•è®ºæ–‡ä¸­çš„è¿™ä¸ªå…¬å¼ä¼¼ä¹æœ‰ç‚¹å°é—®é¢˜ï¼Œåº”è¯¥æ˜¯ $w_{ij} = \\frac{\\exp(b_{ij})}{\\sum_{k=1}^{n} \\exp(b_{ik})}$ï¼Ÿï¼‰\n",
    ">\n",
    "> ç„¶åè®¡ç®— $\\vec{\\mathbf{z}}_j^{\\,h} = \\sum_{i=1}^{m} w_{ij}\\, S_{ij}\\, \\vec{\\mathbf{c}}_i^{\\,l}$ï¼Œå†è¿›è¡Œsquashæ“ä½œï¼ˆ$\\vec{\\mathbf{c}}_j^{\\,h} = \\text{squash}(\\vec{\\mathbf{z}}_j^{\\,h}) = \\frac{\\|\\vec{\\mathbf{z}}_j^{\\,h}\\|^2}{1 + \\|\\vec{\\mathbf{z}}_j^{\\,h}\\|^2} \\frac{\\vec{\\mathbf{z}}_j^{\\,h}}{\\|\\vec{\\mathbf{z}}_j^{\\,h}\\|}$ï¼‰å°±å¾—åˆ°äº†æ›´æ–°åçš„ $\\vec{\\mathbf{c}}_j^{\\,h}$ã€‚è¿™æ ·å°±å®Œæˆäº†ä¸€æ¬¡è¿­ä»£ã€‚ç»è¿‡ç»™å®šçš„è¿­ä»£æ¬¡æ•°åï¼Œ$\\vec{\\mathbf{c}}_j^{\\,h}$ å°±å›ºå®šäº†ï¼Œä½œä¸ºåé¢çš„å±‚çš„è¾“å…¥ã€‚\n",
    ">\n",
    "> #### ä¸ºä»€ä¹ˆå€Ÿé‰´èƒ¶å›Šç½‘ç»œï¼Ÿ\n",
    ">\n",
    "> åœ¨èƒ¶å›Šç½‘ç»œä¸­ï¼Œæ¯ä¸ªèƒ¶å›Šçš„æ–¹å‘ä»£è¡¨ä¸€ä¸ªç‰¹å¾ï¼Œè¿™ä¸ªå‘é‡çš„é•¿åº¦ï¼ˆæ¨¡é•¿ï¼‰ä»£è¡¨è¿™ä¸ªç‰¹å¾å­˜åœ¨çš„æ¦‚ç‡ã€‚åœ¨æ¨èç³»ç»Ÿçš„å¬å›ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å…³å¿ƒçš„å°±æ˜¯ç”¨æˆ·çš„å„ä¸ªå…´è¶£çš„è¡¨ç¤ºä»¥åŠå…´è¶£æ˜¯å¦å­˜åœ¨çš„é—®é¢˜ï¼Œå› æ­¤æœ‰å¦‚ä¸‹å¯¹åº”å…³ç³»ï¼š\n",
    ">\n",
    "> - æ¯ä¸ªèƒ¶å›Š $\\leftrightarrow$ ç”¨æˆ·çš„æŸä¸ªå…´è¶£ç‰¹å¾ï¼ˆæ¯”å¦‚ï¼šå–œæ¬¢æ•°ç äº§å“ï¼‰\n",
    "> - èƒ¶å›Šçš„æ¨¡é•¿ $\\leftrightarrow$ è¯¥å…´è¶£å­˜åœ¨çš„å¯èƒ½æ€§\n",
    ">\n",
    "> æ³¨æ„ï¼šæˆ‘ä»¬å·²çŸ¥çš„æ˜¯ç‰©å“å‘é‡ï¼Œå¸Œæœ›å¾—åˆ°çš„æ˜¯ç”¨æˆ·çš„å…´è¶£è¡¨ç¤ºï¼Œå› æ­¤æ¯ä¸ªç‰©å“å‘é‡å¯¹åº”ä¸‹å±‚çš„æ¯ä¸ªèƒ¶å›Šï¼Œç”¨æˆ·çš„æ¯ä¸ªå…´è¶£è¡¨ç¤ºæ˜¯ä¸Šå±‚çš„æ¯ä¸ªèƒ¶å›Šã€‚\n",
    "\n",
    "ä¸ºäº†é€‚åº”å¬å›ä»»åŠ¡ï¼Œå¯¹åŸå§‹çš„åŠ¨æ€è·¯ç”±æœºåˆ¶è¿›è¡Œäº†å¦‚ä¸‹æ”¹åŠ¨ï¼š\n",
    "\n",
    "- å…±äº« $S_{ij}$ çŸ©é˜µï¼Œå³æ‰€æœ‰çš„ $S_{ij}$ éƒ½ç›¸åŒï¼ˆç­‰äº $S$ï¼‰ã€‚åŸå› æœ‰ä¸¤ä¸ªï¼š\n",
    "\n",
    "  - å¯¹ä¸åŒçš„ç”¨æˆ· $u$ï¼Œ$\\mathcal{I_u}$ çš„å¤§å°å¯èƒ½å·®åˆ«å¾ˆå¤§ï¼ˆ$m$ çš„å·®åˆ«å¯èƒ½å¾ˆå¤§ï¼‰ï¼Œå¦‚æœ $S_{ij}$ ä¸å…±äº«ï¼Œæ³›åŒ–æ€§èƒ½ä¸€èˆ¬ä¸å¥½\n",
    "  - æˆ‘ä»¬å¸Œæœ›å„ä¸ªèƒ¶å›Šæ˜¯åœ¨åŒä¸€ä¸ªå‘é‡ç©ºé—´çš„ï¼Œä½†æ˜¯ä¸åŒçš„ $S_{ij}$ çŸ©é˜µä¼šå°†èƒ¶å›Šæ˜ å°„åˆ°ä¸åŒçš„å‘é‡ç©ºé—´\n",
    "\n",
    "  è¿™æ ·ï¼Œè·¯ç”±çš„logitå°±å¯ä»¥é€šè¿‡å¦‚ä¸‹æ–¹å¼æ›´æ–°ï¼š\n",
    "  $$\n",
    "  b_{ij} = b_{ij} + \\vec{\\mathbf{u}}_j^T S \\vec{\\mathbf{e}}_i, \\quad i \\in \\mathcal{I}_u, j \\in \\{1, \\dots, K\\}\n",
    "  $$\n",
    "  å…¶ä¸­ $\\vec{\\mathbf{u}}_j$ æ˜¯è¡¨ç¤ºç”¨æˆ·å…´è¶£ $j$ çš„èƒ¶å›Šï¼Œ$\\vec{\\mathbf{e}}_i$ æ˜¯ä¹‹å‰æåˆ°è¿‡çš„ç‰©å“å‘é‡ï¼ˆå¯¹åº”äºç‰©å“èƒ¶å›Šï¼‰ï¼Œä¸¤ä¸ªå‘é‡çš„ç»´åº¦éƒ½æ˜¯ $d$ã€‚\n",
    "\n",
    "- éšæœºåˆå§‹åŒ– $b_{ij}$ã€‚åŸå§‹çš„åšæ³•æ˜¯å°†æ‰€æœ‰ $b_{ij}$ éƒ½åˆå§‹åŒ–ä¸º0ï¼Œä½†æ˜¯ç”±äº $S$ æ˜¯å…±äº«çš„ï¼Œè¿™ä¼šå¯¼è‡´åç»­åœ¨æ›´æ–°æ—¶æ‰€æœ‰çš„èƒ¶å›Šå–å€¼æ€»æ˜¯ç›¸åŒï¼Œå› æ­¤ç”¨ $b_{ij}\\sim N(0, \\sigma^2)$ è¿›è¡Œéšæœºé‡‡æ ·ã€‚\n",
    "\n",
    "- åŠ¨æ€çš„å…´è¶£èƒ¶å›Šä¸ªæ•°ã€‚æœ‰çš„ç”¨æˆ·çš„å…´è¶£å¯èƒ½æœ¬èº«å°±æ¯”è¾ƒâ€œå¹¿æ³›â€ï¼Œå…¶å®ƒç”¨æˆ·æ„Ÿå…´è¶£çš„å†…å®¹å¯èƒ½ç›¸å¯¹å•ä¸€ï¼Œå› æ­¤ä¸åŒçš„ç”¨æˆ·çš„ $K$ çš„å–å€¼é€šå¸¸å¾ˆéš¾æ˜¯å®Œå…¨ä¸€æ ·çš„ï¼Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„å¯å‘å¼æ¥è‡ªé€‚åº”åœ°è°ƒæ•´ $K$ï¼Œä¸ºä¸åŒçš„ç”¨æˆ· $u$ è®¾ç½®ä¸åŒçš„ $K_u'$ï¼š\n",
    "  $$\n",
    "  K'_u = \\max(1, \\min(K, \\log_2(|\\mathcal{I}_u|)))\n",
    "  $$\n",
    "  å¯¹äºå…´è¶£è¾ƒä¸ºå•ä¸€çš„ç”¨æˆ·ï¼Œè¿™å¯ä»¥èŠ‚çœå¾ˆå¤šçš„è®¡ç®—å’Œå­˜å‚¨å¼€é”€ã€‚ï¼ˆPSï¼šåœ¨å…·ä½“å®ç°å±‚é¢ï¼ŒåŠ¨æ€çš„ $K$ æ˜¯é€šè¿‡æ©ç æ¥å®ç°çš„ï¼‰\n",
    "\n",
    "### Label-aware Attention Layer\n",
    "\n",
    "æ¯ä¸ªèƒ¶å›Šå¯¹åº”çš„å‘é‡ä»£è¡¨ç”¨æˆ·çš„ä¸€ä¸ªå…´è¶£æ–¹é¢ï¼Œåœ¨è¯„ä¼°ç”¨æˆ·å¯¹æŸä¸ªå…·ä½“ç‰©å“çš„å…´è¶£æ—¶ï¼Œåº”è¯¥ä½¿ç”¨æœ€ç›¸å…³çš„é‚£ä¸ªèƒ¶å›Šï¼ŒMINDé€šè¿‡label-aware attentionå±‚ï¼ˆåŸºäºå°ºåº¦å½’ä¸€åŒ–åçš„å‘é‡å†…ç§¯ï¼‰æ¥æ‰¾åˆ°å¯¹åº”çš„èƒ¶å›Šã€‚\n",
    "\n",
    "å¯¹ç»™å®šçš„target item $i$ï¼Œå°†å…¶å¯¹åº”çš„ç‰©å“å‘é‡ $\\vec{\\mathbf{e}}_i$ ä½œä¸ºqueryï¼Œå°†å¾—åˆ°çš„æ¯ä¸ªèƒ¶å›Šå‘é‡ $\\vec{\\mathbf{u}}_j$ ä¸ $\\vec{\\mathbf{p}}_u$ æ‹¼æ¥å¾—åˆ°çš„å‘é‡ $\\vec{\\mathbf{v}}_u^j$ ä½œä¸ºkeyså’Œvaluesï¼Œå¾—åˆ°å…³äºè¿™ä¸ªç‰©å“çš„ç”¨æˆ·å…´è¶£çš„æœ€ç»ˆè¡¨ç¤ºå‘é‡ï¼š\n",
    "$$\n",
    "\\begin{aligned} \\vec{\\mathbf{v}}_u &= \\text{Attention}(\\vec{\\mathbf{e}}_i, V_u, V_u) \\\\ &= V_u \\text{softmax}(\\text{pow}(V_u^\\mathrm{T} \\vec{\\mathbf{e}}_i, p)) \\end{aligned}\n",
    "$$\n",
    "å…¶ä¸­ $p$ æ˜¯ç”¨äºè°ƒæ•´æ³¨æ„åŠ›çš„â€œè½¯ç¡¬ç¨‹åº¦â€çš„è¶…å‚æ•°ï¼Œå½“ $p$ è¶‹äºæ— ç©·å¤§æ—¶ï¼Œlogitæœ€å¤§çš„èƒ¶å›Šçš„æƒé‡å‡ ä¹ä¸º1ï¼Œå…¶å®ƒçš„èƒ¶å›Šå‡ ä¹å®Œå…¨è¢«å¿½ç•¥ï¼Œå®éªŒæµ‹è¯•å‘ç°æ­¤æ—¶èƒ½å¾ˆå¿«æ”¶æ•›ï¼›$V_u = \\left( \\vec{\\mathbf{v}}_u^1, \\dots, \\vec{\\mathbf{v}}_u^K \\right) \\in \\mathbb{R}^{d \\times K}$ã€‚\n",
    "\n",
    "### Training\n",
    "\n",
    "åœ¨å¾—åˆ°æœ€ç»ˆçš„ç”¨æˆ·å…´è¶£å‘é‡ $\\vec{\\mathbf{v}}_u$ å’Œç‰©å“å‘é‡ $\\vec{\\mathbf{e}}_i$ åï¼Œå¯ä»¥è®¡ç®—ç”¨æˆ· $u$ ä¸ç‰©å“ $i$ çš„äº¤äº’æ¦‚ç‡ï¼ˆç”±äºç‰©å“æ€»æ•°å¾ˆå¤§ï¼Œå®é™…ä¸Šä¼šç”¨sampled softmaxæ¥æ›¿ä»£softmaxï¼‰ï¼š\n",
    "$$\n",
    "\\operatorname{Pr}(i|u) = \\operatorname{Pr}(\\vec{\\mathbf{e}}_i|\\vec{\\mathbf{v}}_u) = \\frac{\\exp \\left(\\vec{\\mathbf{v}}_u^\\mathrm{T} \\vec{\\mathbf{e}}_i\\right)}{\\sum_{j \\in \\mathcal{I}} \\exp \\left(\\vec{\\mathbf{v}}_u^\\mathrm{T} \\vec{\\mathbf{e}}_j\\right)}\n",
    "$$\n",
    "MINDçš„æ€»çš„è®­ç»ƒç›®æ ‡æ˜¯æœ€å¤§åŒ–ä¸‹é¢çš„ç›®æ ‡å‡½æ•°ï¼š\n",
    "$$\n",
    "L = \\sum_{(u, i) \\in \\mathcal{D}} \\log \\operatorname{Pr}(i|u)\n",
    "$$\n",
    "å…¶ä¸­ $\\mathcal{D}$ æ˜¯è®­ç»ƒæ•°æ®ä¸­æ‰€æœ‰å·²å‘ç”Ÿçš„ç”¨æˆ·-ç‰©å“äº¤äº’ã€‚\n",
    "\n",
    "### Inference (Serving)\n",
    "\n",
    "å®é™…æ¨ç†æ—¶ï¼Œé™¤äº†label-aware attentionå±‚ï¼ŒMINDç½‘ç»œçš„å…¶å®ƒéƒ¨åˆ†ä»…ä¸ç”¨æˆ·ç›¸å…³ï¼Œå¯¹æ¯ä¸ªç”¨æˆ·ï¼Œè®¡ç®—å®Œ $V_u = \\left( \\vec{\\mathbf{v}}_u^1, \\dots, \\vec{\\mathbf{v}}_u^K \\right)$ åï¼Œå¯ä»¥ç”¨ANNæ‰¾åˆ°å…¶top-Nçš„æœ€è¿‘é‚»ï¼Œå°±æ˜¯æˆ‘ä»¬è¦æ¨èçš„ç‰©å“ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬è¦æ‰¾çš„å°±æ˜¯ä¸‹é¢çš„åˆ†æ•°ï¼ˆè¿‘ä¼¼ï¼‰æœ€å¤§çš„Nä¸ªç‰©å“ï¼š\n",
    "$$\n",
    "f_{score}(V_u, \\vec{\\mathbf{e}}_i) = \\max_{1 \\le k \\le K} \\vec{\\mathbf{e}}_i^\\mathrm{T} \\vec{\\mathbf{v}}_u^k\n",
    "$$\n",
    "æ³¨æ„ï¼šæ¯å½“ç”¨æˆ· $u$ æœ‰æ–°çš„äº¤äº’è¡Œä¸ºåï¼Œä»–çš„äº¤äº’å†å² $\\mathcal{I_u}$ å°±ä¼šå˜åŒ–ï¼Œä»è€Œä¼šæœ‰æ–°çš„ç”¨æˆ·è¡¨ç¤ºå‘é‡ï¼ŒMINDç”±æ­¤å¯ä»¥è¿›è¡Œ**å®æ—¶**çš„ä¸ªæ€§åŒ–æ¨èã€‚æ¨ç†é˜¶æ®µæ˜¯æ²¡æœ‰è®¡ç®—label-aware attentionçš„ã€‚\n",
    "\n",
    "## 3. MINDç½‘ç»œä¸å…¶å®ƒç½‘ç»œçš„å…³è”\n",
    "\n",
    "1. *YouTube DNN*: å½“MINDçš„èƒ¶å›Šæ•° $K=1$ æ—¶ï¼ŒMINDå°±é€€åŒ–ä¸º *YouTube DNN*ã€‚\n",
    "2. *DIN*: DINå’ŒMINDéƒ½è‡´åŠ›äºæ•æ‰ç”¨æˆ·å…´è¶£çš„å¤šä¸ªå±‚é¢ï¼Œä½†æ˜¯DINæ˜¯åœ¨ç‰©å“å±‚é¢ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶ï¼Œè€ŒMINDæ˜¯åœ¨å…´è¶£å±‚é¢ä½¿ç”¨äº†æ³¨æ„åŠ›æœºåˆ¶ï¼›DINä¸»è¦ç”¨äºé‡æ’é˜¶æ®µï¼Œæ­¤æ—¶å¤„ç†çš„æ˜¯åƒçº§çš„ç‰©å“ï¼Œä½†æ˜¯MINDé€šè¿‡å¯¹ç”¨æˆ·å’Œç‰©å“çš„è¡¨ç¤ºè¿›è¡Œè§£è€¦ï¼Œå¯ä»¥ç”¨äºéœ€è¦é¢å¯¹äº¿çº§äº§å“çš„å¬å›é—®é¢˜ã€‚\n",
    "\n",
    "## 4. ä¸šåŠ¡æ´å¯Ÿ\n",
    "\n",
    "1. å½“æ—¶å¤§å¤šæ•°çš„æ·±åº¦å­¦ä¹ æ¨èæ¨¡å‹ï¼ˆå¦‚ YouTube DNN ç­‰ï¼‰é€šå¸¸ç”¨ä¸€ä¸ªå•ä¸€çš„å›ºå®šé•¿åº¦å‘é‡æ¥è¡¨ç¤ºä¸€ä¸ªç”¨æˆ·ï¼Œä½†ç”¨æˆ·çš„å…´è¶£å¾€å¾€æ˜¯**å¤šæ ·ä¸”å¤šé¢æ€§çš„**ï¼Œå°†æ‰€æœ‰è¿™äº›ä¸åŒçš„å…´è¶£å¼ºè¡Œå‹ç¼©åˆ°ä¸€ä¸ªå‘é‡ä¸­ï¼Œä¼šå¯¼è‡´ä¿¡æ¯çš„ä¸¥é‡ä¸¢å¤±ï¼Œæ— æ³•å‡†ç¡®æ•æ‰ç”¨æˆ·å…´è¶£çš„ä¸°å¯Œå˜åŒ–ï¼Œä»è€Œé™åˆ¶æ¨èç³»ç»Ÿçš„å¬å›èƒ½åŠ›ã€‚\n",
    "2. **åŒ¹é…ï¼ˆå¬å›ï¼‰é˜¶æ®µå†³å®šäº†æ¨èæ•ˆæœçš„ä¸Šé™**ã€‚å¦‚æœåŒ¹é…é˜¶æ®µæ— æ³•å¬å›ç”¨æˆ·æ„Ÿå…´è¶£çš„â€œå°ä¼—â€æˆ–â€œå³æ—¶â€å…´è¶£å•†å“ï¼Œåç»­çš„æ’åºé˜¶æ®µæ¨¡å‹å†å¼ºä¹Ÿæ— æµäºäº‹ã€‚å› æ­¤ï¼Œåœ¨åŒ¹é…é˜¶æ®µå°±å¼•å…¥å¤šå…´è¶£å»ºæ¨¡æ˜¯æå‡æ•´ä½“æ•ˆæœçš„å…³é”®ã€‚\n",
    "\n",
    "## 5. ä»£ç å®ç°Comments\n",
    "\n",
    "### âœ¨ Features\n",
    "\n",
    "- âœ… æ”¯æŒç”¨æˆ·é™æ€ç‰¹å¾ï¼ˆå³æ¶æ„å›¾ä¸­çš„*Other Features* / è®ºæ–‡ä¸­çš„*user profile*ï¼Œå¦‚ç”¨æˆ·å¹´é¾„å’Œæ€§åˆ«ï¼‰\n",
    "- âœ… æ”¯æŒç‰©å“é™æ€ç‰¹å¾\n",
    "- âœ… æ”¯æŒæ ¹æ®ç”¨æˆ·äº¤äº’å†å²åŠ¨æ€è°ƒæ•´ $K$ å€¼ï¼ˆmask capsulesï¼‰\n",
    "- âœ… æ”¯æŒå¯¹ç”¨æˆ·äº¤äº’å†å²ä¸­è¡¨ç¤ºpaddingçš„itemè¿›è¡Œæ©ç ï¼ˆmask itemsï¼‰\n",
    "\n",
    "ä¸ºçªå‡ºä¸»è¦é€»è¾‘ï¼Œæµ‹è¯•demoæœ‰ä¸€å®šçš„ç®€åŒ–ï¼š\n",
    "\n",
    "1. å¬å›topKä¸ªç‰©å“æ—¶ï¼Œè®¡ç®—äº†æ‰€æœ‰å€™é€‰ç‰©å“çš„logitï¼Œæ²¡æœ‰ç”¨ANNã€‚\n",
    "\n",
    "## 6. å‚è€ƒå†…å®¹é“¾æ¥\n",
    "\n",
    "1. [Multi-Interest Network with Dynamic Routing for Recommendation at Tmall](https://arxiv.org/pdf/1904.08030)\n",
    "2. [FunRecæ¨èç³»ç»Ÿ 2.3.1.1èŠ‚]([2.3.1. æ·±åŒ–ç”¨æˆ·å…´è¶£è¡¨ç¤º â€” FunRec æ¨èç³»ç»Ÿ 0.0.1 documentation](https://datawhalechina.github.io/fun-rec/chapter_1_retrieval/3.sequence/1.user_interests.html))\n",
    "3. https://github.com/Wang-Yu-Qing/MIND\n",
    "4. https://github.com/danielhavir/capsule-network\n"
   ],
   "id": "543dc60cb07986d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MIND(nn.Module):\n",
    "\n",
    "    def __init__(self, d: int, K: int, user_profile_embed_dim: dict,\n",
    "                 num_items: int, max_len: int, num_neg: int,\n",
    "                 iter_num: int=3, device: str='cpu'):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            d: dimension of item vec\n",
    "            K: number of capsules\n",
    "            user_profile_embed_dim: a dict specifying the embedding dimension of each feature in user profile\n",
    "            num_items: size of item pool\n",
    "            max_len: max length of user behavior sequence\n",
    "            num_neg: number of negative samples per positive\n",
    "            iter_num: number of iterations in capsule calculation\n",
    "            device: device to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.num_items = num_items\n",
    "        self.max_len = max_len\n",
    "        self.iter_num = iter_num\n",
    "        self.num_neg = num_neg\n",
    "        self.device = device\n",
    "\n",
    "        # Embedding layer\n",
    "        self.itemEmbeds = nn.Embedding(num_items, d, padding_idx=0)\n",
    "        self.userEmbeds = nn.ModuleDict()\n",
    "        self.user_profile_dim = 0\n",
    "        for feat_name, (n_cat, embed_dim) in user_profile_embed_dim.items():\n",
    "            self.userEmbeds[feat_name] = nn.Embedding(n_cat, embed_dim)\n",
    "            self.user_profile_dim += embed_dim\n",
    "        self.dense1 = nn.Linear(self.user_profile_dim + d, 4 * d)\n",
    "        self.dense2 = nn.Linear(4 * d, d)\n",
    "\n",
    "        # Shared bilinear mapping matrix\n",
    "        self.S = nn.Parameter(torch.empty(d, d, device=device))\n",
    "        nn.init.normal_(self.S, mean=0.0, std=1.0)\n",
    "\n",
    "    def forward(self, history, user_profile):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            history: [batch_size, max_len]\n",
    "            user_profile: a dict, each value is of shape [batch_size, 1]\n",
    "        Returns:\n",
    "            tensor: [batch_size, K, d]\n",
    "        \"\"\"\n",
    "        batch_size = history.shape[0]\n",
    "        # initialize logits of capsule pairs\n",
    "        B = torch.empty(self.K, self.max_len, device=self.device)       # [K, max_len]\n",
    "        nn.init.normal_(B, mean=0.0, std=1.0)\n",
    "        B = B.repeat(batch_size, 1, 1)   # [batch_size, K, max_len]\n",
    "        # masking: ensure padding item capsules do not affect high-level capsules\n",
    "        mask = (history == 0).unsqueeze(1)      # [batch_size, 1, max_len]\n",
    "\n",
    "        # perform embedding\n",
    "        item_embeds = self.itemEmbeds(history)  # [batch_size, max_len, d]\n",
    "\n",
    "        # avoid calculate this repeatedly\n",
    "        Sc = torch.matmul(item_embeds, self.S)  # [batch_size, max_len, d]\n",
    "\n",
    "        # update capsules in a iterative way\n",
    "        for i in range(self.iter_num):\n",
    "            B_masked = B.masked_fill(mask, -1e9)    # [batch_size, K, max_len]\n",
    "            W = F.softmax(B_masked, dim=1)          # [batch_size, K, max_len]\n",
    "            caps = torch.matmul(W, Sc)      # [batch_size, K, d]\n",
    "            caps = self.squash(caps)        # [batch_size, K, d]\n",
    "            # update B\n",
    "            B = torch.matmul(caps, Sc.transpose(1, 2))     # [batch_size, K, max_len]\n",
    "\n",
    "        # user embedding\n",
    "        user_embeds = torch.empty(batch_size, 0, device=self.device)\n",
    "        for feat in user_profile:\n",
    "            user_embeds = torch.cat([user_embeds, self.userEmbeds[feat](user_profile[feat])], dim=1)\n",
    "\n",
    "        # concatenate user profile vec with caps\n",
    "        caps = torch.cat([caps, user_embeds.unsqueeze(1).repeat(1, 3, 1)], dim=-1)\n",
    "        caps = self.dense2(F.relu(self.dense1(caps)))    # [batch_size, K, d]\n",
    "\n",
    "        return caps\n",
    "\n",
    "    def squash(self, caps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            caps: [batch_size, K, d]\n",
    "        Returns:\n",
    "            tensor: [batch_size, K, d]\n",
    "        \"\"\"\n",
    "        norm = torch.norm(caps, dim=2, keepdim=True)    # [batch_size, K, 1]\n",
    "        squared_norm = torch.pow(norm, 2)               # [batch_size, K, 1]\n",
    "\n",
    "        return (squared_norm / ((1 + squared_norm) * norm + 1e-9)) * caps\n",
    "\n",
    "    def layer_aware_attention(self, caps, target_items, p=2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            caps: [batch_size, K, d]\n",
    "            target_items: [batch_size, self.num_neg + 1]\n",
    "        Returns:\n",
    "            tensor: [batch_size, self.num_neg + 1, d]\n",
    "        \"\"\"\n",
    "        target_item_embeds = self.itemEmbeds(target_items)    # [batch_size, self.num_neg + 1, d]\n",
    "        attn_score = torch.matmul(target_item_embeds, caps.transpose(1, 2))  # [batch_size, self.num_neg + 1, K]\n",
    "        attn_score = torch.pow(attn_score, p)   # [batch_size, self.num_neg + 1, K]\n",
    "\n",
    "        v_user = torch.matmul(F.softmax(attn_score, dim=2), caps)  # [batch_size, self.num_neg + 1, d]\n",
    "\n",
    "        score = (v_user * target_item_embeds).sum(dim=-1)          # [batch_size, self.num_neg + 1]\n",
    "\n",
    "        return score\n",
    "\n",
    "    def sampled_softmax(self, caps, target_item, history, p=2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            caps: [batch_size, K, d]\n",
    "            target_item: [batch_size, 1]\n",
    "            history: [batch_size, max_len]\n",
    "            p: float\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size = caps.shape[0]\n",
    "        # negative items should not be in the history as well as be the target item\n",
    "        # to avoid loop and do calculation efficiently, when the unwanted negative items are included, mask them\n",
    "        neg_items = torch.randint(1, self.num_items, (batch_size, self.num_neg), device=self.device)    # [batch_size, self.num_neg]\n",
    "        target_items = torch.cat([target_item, neg_items], dim=1)   # [batch_size, self.num_neg + 1]\n",
    "\n",
    "        # check validity of \"negative\" samples\n",
    "        # history.unsqueeze(1): [batch_size, 1, max_len]\n",
    "        # neg_items.unsqueeze(-1): [batch_size, self.num_neg, 1]\n",
    "        history_hit = (history.unsqueeze(1) == neg_items.unsqueeze(-1))     # [batch_size, self.num_neg, max_len]\n",
    "        mask = history_hit.any(dim=-1) | (neg_items == target_item)     # [batch_size, self.num_neg]\n",
    "        mask = torch.concat([torch.zeros((batch_size, 1), dtype=torch.bool, device=mask.device), mask], dim=1)  # [batch_size, self.num_neg + 1]\n",
    "\n",
    "        # calculate logits\n",
    "        logits = self.layer_aware_attention(caps, target_items, p=p)    # [batch_size, self.num_neg + 1]\n",
    "        logits_masked = logits.masked_fill(mask, -1e9)                  # [batch_size, self.num_neg + 1]\n",
    "        labels = torch.concat([torch.ones(batch_size, 1), torch.zeros(batch_size, self.num_neg)], dim=1)  # [batch_size, self.num_neg + 1]\n",
    "\n",
    "        return logits_masked, labels\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
